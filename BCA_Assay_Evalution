-----------------------------
title: "BCA Assay Evalution"
author: "Jakubec, Martin, PhD."
-----------------------------

# ============================================================================
# DATA IMPORT MODULE FOR GOOGLE COLAB
# Converting Excel Plate Reader Data to Long Format
# ============================================================================

import numpy as np
import pandas as pd
import os
from google.colab import files
from google.colab import drive
import io

# ============================================================================
# GOOGLE COLAB SETUP AND FILE HANDLING
# ============================================================================

def setup_colab_environment():
    """
    Setup function for Google Colab environment.
    Provides options for file upload and Google Drive mounting.
    """
    print("üîß Setting up Google Colab environment...")
    print("\nChoose your data source:")
    print("1. Upload files directly to Colab")
    print("2. Mount Google Drive and access files")
    print("3. Use sample data for testing")

    choice = input("\nEnter your choice (1, 2, or 3): ")

    if choice == "1":
        return setup_file_upload()
    elif choice == "2":
        return setup_google_drive()
    elif choice == "3":
        return setup_sample_data()
    else:
        print("Invalid choice. Defaulting to file upload.")
        return setup_file_upload()

def setup_file_upload():
    """
    Handle direct file upload to Colab.
    """
    print("\nüìÅ Upload your Excel files:")
    print("Click 'Choose Files' and select your Excel file(s)")

    uploaded = files.upload()

    if uploaded:
        file_paths = list(uploaded.keys())
        print(f"\n‚úÖ Successfully uploaded {len(file_paths)} file(s):")
        for path in file_paths:
            print(f"   - {path}")
        return file_paths[0]  # Return first file path
    else:
        print("‚ùå No files uploaded.")
        return None

def setup_google_drive():
    """
    Mount Google Drive and navigate to data folder.
    """
    print("\nüîó Mounting Google Drive...")
    drive.mount('/content/drive')

    print("\nüìÇ Google Drive mounted at /content/drive/")
    print("Your files are located at: /content/drive/MyDrive/")
    print("\nExample path: /content/drive/MyDrive/YourDataFolder/your_file.xlsx")

    # Allow user to specify their data folder
    data_folder = input("\nEnter the path to your data folder (or press Enter for default): ")
    if not data_folder:
        data_folder = "/content/drive/MyDrive/"

    return data_folder

def setup_sample_data():
    """
    Create sample data for testing purposes.
    """
    print("\nüß™ Creating sample data for testing...")

    # Create sample absorbance data (8x12 plate)
    np.random.seed(42)  # For reproducible results
    abs_data = pd.DataFrame(
        np.random.uniform(0.1, 2.0, (8, 12)),
        index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],
        columns=list(range(1, 13))
    )

    # Create sample ID data
    id_data = pd.DataFrame(
        [[f"Sample_{i}_{j}" for j in range(1, 13)] for i in range(8)],
        index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],
        columns=list(range(1, 13))
    )

    # Save to Excel file
    sample_file = "sample_plate_data.xlsx"
    with pd.ExcelWriter(sample_file, engine='openpyxl') as writer:
        abs_data.to_excel(writer, sheet_name='Sheet1', startrow=1, startcol=1)
        id_data.to_excel(writer, sheet_name='Sheet1', startrow=11, startcol=1)

    print(f"‚úÖ Sample data created: {sample_file}")
    return sample_file

  # ============================================================================
# MAIN DATA IMPORT FUNCTION (ENHANCED FOR COLAB)
# ============================================================================

def import_and_reshape_excel_colab(file_path, sheet_name=0,
                                   abs_range='B2:M9', id_range='B12:M19',
                                   validate_data=True):
    """
    Enhanced version of the Excel import function optimized for Google Colab.

    This function reads plate reader data from Excel files and converts it to long format,
    which is more suitable for data analysis and visualization.

    Parameters:
    -----------
    file_path : str
        Path to Excel file (local file or Google Drive path)
    sheet_name : str or int, default 0
        Sheet name or index to read from
    abs_range : str, default 'B2:M9'
        Excel range for absorbance data (standard 8x12 plate: rows A-H, cols 1-12)
    id_range : str, default 'B12:M19'
        Excel range for sample ID data
    validate_data : bool, default True
        Whether to perform data validation checks

    Returns:
    --------
    pandas.DataFrame
        Long format dataframe with columns: Well, Row, Column, Absorbance, ID

    Example:
    --------
    >>> data = import_and_reshape_excel_colab('plate_data.xlsx')
    >>> print(data.head())
    """

    print(f"üìä Processing file: {file_path}")

    try:
        # Parse Excel ranges
        abs_start_col = abs_range.split(':')[0][0]
        abs_end_col = abs_range.split(':')[1][0]
        abs_start_row = int(abs_range.split(':')[0][1:]) - 1
        abs_end_row = int(abs_range.split(':')[1][1:])

        id_start_col = id_range.split(':')[0][0]
        id_end_col = id_range.split(':')[1][0]
        id_start_row = int(id_range.split(':')[0][1:]) - 1
        id_end_row = int(id_range.split(':')[1][1:])

        # Read absorbance data
        print("   Reading absorbance data...")
        abs_data = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            usecols=f"{abs_start_col}:{abs_end_col}",
            skiprows=abs_start_row,
            nrows=abs_end_row - abs_start_row,
            header=None
        )

        # Read ID data
        print("   Reading sample ID data...")
        id_data = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            usecols=f"{id_start_col}:{id_end_col}",
            skiprows=id_start_row,
            nrows=id_end_row - id_start_row,
            header=None
        )

        # Create standard plate labels
        rows = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']
        columns = list(range(1, 13))  # 1-12 for standard 96-well plate

        # Set proper index and column names
        abs_data.index = rows[:len(abs_data)]
        abs_data.columns = columns[:len(abs_data.columns)]
        id_data.index = rows[:len(id_data)]
        id_data.columns = columns[:len(id_data.columns)]

        # Data validation
        if validate_data:
            print("   Validating data...")
            validate_plate_data(abs_data, id_data)

        # Convert to long format
        print("   Converting to long format...")

        # Reshape absorbance data
        abs_long = abs_data.reset_index().melt(
            id_vars='index',
            var_name='Column',
            value_name='Absorbance'
        )
        abs_long.rename(columns={'index': 'Row'}, inplace=True)

        # Reshape ID data
        id_long = id_data.reset_index().melt(
            id_vars='index',
            var_name='Column',
            value_name='ID'
        )
        id_long.rename(columns={'index': 'Row'}, inplace=True)

        # Combine data
        combined_data = pd.merge(abs_long, id_long, on=['Row', 'Column'], how='outer')

        # Create well identifiers (e.g., A1, B2, etc.)
        combined_data['Well'] = combined_data['Row'] + combined_data['Column'].astype(str)

        # Reorder columns for clarity
        combined_data = combined_data[['Well', 'Row', 'Column', 'Absorbance', 'ID']]

        # Sort by well for consistent ordering
        combined_data = combined_data.sort_values(['Row', 'Column']).reset_index(drop=True)

        print("‚úÖ Data successfully imported and reshaped!")
        print(f"   Shape: {combined_data.shape}")
        print(f"   Wells: {combined_data['Well'].nunique()}")
        print(f"   Samples: {combined_data['ID'].nunique()}")

        return combined_data

    except FileNotFoundError:
        print(f"‚ùå Error: File '{file_path}' not found.")
        print("üí° Make sure the file path is correct and the file exists.")
        return None

    except Exception as e:
        print(f"‚ùå Error occurred: {str(e)}")
        print("üí° Check your Excel file format and range specifications.")
        return None

# ============================================================================
# DATA VALIDATION FUNCTIONS
# ============================================================================

def validate_plate_data(abs_data, id_data):
    """
    Validate the imported plate data for common issues.

    Parameters:
    -----------
    abs_data : pandas.DataFrame
        Absorbance data in wide format
    id_data : pandas.DataFrame
        Sample ID data in wide format
    """

    issues = []

    # Check for consistent dimensions
    if abs_data.shape != id_data.shape:
        issues.append(f"Dimension mismatch: Absorbance {abs_data.shape} vs ID {id_data.shape}")

    # Check for missing values
    abs_missing = abs_data.isnull().sum().sum()
    id_missing = id_data.isnull().sum().sum()

    if abs_missing > 0:
        issues.append(f"Missing absorbance values: {abs_missing}")

    if id_missing > 0:
        issues.append(f"Missing sample IDs: {id_missing}")

    # Check for non-numeric absorbance values
    try:
        pd.to_numeric(abs_data.values.flatten(), errors='raise')
    except (ValueError, TypeError):
        issues.append("Non-numeric values found in absorbance data")

    # Report issues
    if issues:
        print("‚ö†Ô∏è  Data validation warnings:")
        for issue in issues:
            print(f"   - {issue}")
    else:
        print("‚úÖ Data validation passed!")

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def preview_excel_file(file_path, sheet_name=0, max_rows=10, max_cols=10):
    """
    Preview an Excel file to help determine the correct ranges.

    Parameters:
    -----------
    file_path : str
        Path to Excel file
    sheet_name : str or int
        Sheet to preview
    max_rows : int
        Maximum rows to display
    max_cols : int
        Maximum columns to display
    """

    try:
        print(f"üìã Previewing file: {file_path}")

        # Read first few rows and columns
        preview_data = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            nrows=max_rows,
            usecols=range(max_cols),
            header=None
        )

        print(f"\nFirst {max_rows} rows and {max_cols} columns:")
        print(preview_data.to_string())

        # Show sheet names if multiple sheets exist
        xl_file = pd.ExcelFile(file_path)
        if len(xl_file.sheet_names) > 1:
            print(f"\nAvailable sheets: {xl_file.sheet_names}")

    except Exception as e:
        print(f"‚ùå Error previewing file: {str(e)}")

def save_data_colab(data, filename="processed_data.csv", to_drive=False):
    """
    Save processed data in Google Colab environment.

    Parameters:
    -----------
    data : pandas.DataFrame
        Data to save
    filename : str
        Output filename
    to_drive : bool
        Whether to save to Google Drive (requires mounting)
    """

    if to_drive:
        save_path = f"/content/drive/MyDrive/{filename}"
        print(f"üíæ Saving to Google Drive: {save_path}")
    else:
        save_path = f"/content/{filename}"
        print(f"üíæ Saving to Colab storage: {save_path}")

    try:
        data.to_csv(save_path, index=False)
        print(f"‚úÖ Data saved successfully!")

        if not to_drive:
            print("üí° To download the file, use: files.download(filename)")

    except Exception as e:
        print(f"‚ùå Error saving file: {str(e)}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import linregress, t
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

def filter_and_analyze_standard_curve(long_format_data, concentration_unit='units', measurement_name='Measurement'):
    """
    Filter standard curve data (numeric IDs) and calculate statistics.

    Parameters:
    long_format_data (pd.DataFrame): The long format dataframe with columns ['ID', 'Absorbance'] or similar
    concentration_unit (str): Unit for concentration (e.g., 'Œºg/ml', 'ng/ml', 'mM', 'units')
    measurement_name (str): Name of the measurement (e.g., 'Absorbance', 'Fluorescence', 'Signal')

    Returns:
    tuple: (standard_curve_data, statistics_summary)
    """

    # Create a copy to avoid modifying original data
    df = long_format_data.copy()

    # Identify the measurement column (assume second column if not 'Absorbance')
    measurement_col = 'Absorbance' if 'Absorbance' in df.columns else df.columns[1]

    # Remove rows with missing values
    df = df.dropna(subset=['ID', measurement_col])

    # Function to check if a value is purely numeric (including floats)
    def is_numeric(value):
        try:
            float(value)
            return True
        except (ValueError, TypeError):
            return False

    # Filter for standard curve (numeric IDs only)
    df['is_numeric'] = df['ID'].apply(is_numeric)
    standard_curve_data = df[df['is_numeric'] == True].copy()

    if len(standard_curve_data) == 0:
        print("‚ùå No numeric IDs found for standard curve analysis!")
        return pd.DataFrame(), pd.DataFrame()

    # Convert ID to numeric for proper sorting and analysis
    standard_curve_data['Concentration'] = pd.to_numeric(standard_curve_data['ID'])

    # Ensure measurement is numeric
    standard_curve_data[measurement_col] = pd.to_numeric(standard_curve_data[measurement_col])

    # Calculate statistics for each concentration
    stats_summary = standard_curve_data.groupby('Concentration')[measurement_col].agg([
        'count',  # Number of replicates
        'mean',   # Average
        'std',    # Standard deviation
        'sem',    # Standard error of mean
        'min',    # Minimum value
        'max'     # Maximum value
    ]).round(4)

    # Calculate CV (coefficient of variation) as percentage
    stats_summary['cv_percent'] = (stats_summary['std'] / stats_summary['mean'] * 100).round(2)

    # Reset index to make Concentration a column
    stats_summary = stats_summary.reset_index()

    # Rename columns for clarity
    stats_summary.columns = ['Concentration', 'n_replicates', f'mean_{measurement_col.lower()}',
                           'std_dev', 'std_error', f'min_{measurement_col.lower()}',
                           f'max_{measurement_col.lower()}', 'cv_percent']

    # Sort by concentration
    stats_summary = stats_summary.sort_values('Concentration')

    print("=== STANDARD CURVE ANALYSIS ===")
    print(f"Total wells in dataset: {len(df)}")
    print(f"Standard curve wells (numeric IDs): {len(standard_curve_data)}")
    print(f"Unique concentrations: {len(stats_summary)}")
    print(f"Concentration range: {standard_curve_data['Concentration'].min()} - {standard_curve_data['Concentration'].max()} {concentration_unit}")

    print(f"\n=== STATISTICS SUMMARY ===")
    print(stats_summary.to_string(index=False))

    return standard_curve_data, stats_summary

def fit_linear_standard_curve(stats_summary, concentration_unit='units', measurement_name='Measurement'):
    """
    Fit a linear function to the standard curve data.

    Parameters:
    stats_summary (pd.DataFrame): Statistics summary with mean measurement values
    concentration_unit (str): Unit for concentration
    measurement_name (str): Name of the measurement

    Returns:
    dict: Dictionary containing fit parameters and functions
    """

    # Get the mean measurement column name
    mean_col = [col for col in stats_summary.columns if col.startswith('mean_')][0]

    # Extract data for fitting (remove any NaN values)
    fit_data = stats_summary.dropna(subset=[mean_col])

    if len(fit_data) < 2:
        raise ValueError("Need at least 2 data points for linear fitting")

    x = fit_data['Concentration'].values
    y = fit_data[mean_col].values

    # Perform linear regression using scipy.stats for comprehensive statistics
    slope, intercept, r_value, p_value, std_err = linregress(x, y)

    # Calculate additional statistics
    r_squared = r_value ** 2

    # Calculate confidence intervals for the slope and intercept
    n = len(x)
    dof = n - 2  # degrees of freedom
    t_val = t.ppf(0.975, dof)  # 95% confidence interval

    # Standard error of slope and intercept
    se_slope = std_err
    se_intercept = se_slope * np.sqrt(np.sum(x**2) / n)

    # Confidence intervals
    slope_ci = (slope - t_val * se_slope, slope + t_val * se_slope)
    intercept_ci = (intercept - t_val * se_intercept, intercept + t_val * se_intercept)

    # Create prediction functions
    def predict_concentration(measurement_value):
        """
        Predict concentration from measurement using fitted curve.
        Formula: concentration = (measurement - intercept) / slope
        """
        return (measurement_value - intercept) / slope

    def predict_measurement(concentration):
        """
        Predict measurement from concentration using fitted curve.
        Formula: measurement = slope * concentration + intercept
        """
        return slope * concentration + intercept

    # Store all results
    fit_results = {
        'slope': slope,
        'intercept': intercept,
        'r_squared': r_squared,
        'r_value': r_value,
        'p_value': p_value,
        'std_err_slope': se_slope,
        'std_err_intercept': se_intercept,
        'slope_ci': slope_ci,
        'intercept_ci': intercept_ci,
        'n_points': n,
        'equation_string': f'y = {slope:.4f}x + {intercept:.4f}',
        'equation_formatted': f'{measurement_name} = {slope:.4f} √ó Concentration + {intercept:.4f}',
        'predict_concentration': predict_concentration,
        'predict_measurement': predict_measurement,
        'x_data': x,
        'y_data': y,
        'concentration_unit': concentration_unit,
        'measurement_name': measurement_name
    }

    # Print fit results
    print("\n=== LINEAR FIT RESULTS ===")
    print(f"Equation: {fit_results['equation_formatted']}")
    print(f"R¬≤ = {r_squared:.4f}")
    print(f"R = {r_value:.4f}")
    print(f"p-value = {p_value:.2e}")
    print(f"Slope: {slope:.4f} ¬± {se_slope:.4f} (95% CI: {slope_ci[0]:.4f} to {slope_ci[1]:.4f})")
    print(f"Intercept: {intercept:.4f} ¬± {se_intercept:.4f} (95% CI: {intercept_ci[0]:.4f} to {intercept_ci[1]:.4f})")

    if r_squared >= 0.95:
        print("‚úÖ Excellent fit (R¬≤ ‚â• 0.95)")
    elif r_squared >= 0.90:
        print("‚úÖ Good fit (R¬≤ ‚â• 0.90)")
    elif r_squared >= 0.80:
        print("‚ö†Ô∏è  Acceptable fit (R¬≤ ‚â• 0.80)")
    else:
        print("‚ùå Poor fit (R¬≤ < 0.80) - consider checking data quality")

    return fit_results

def plot_standard_curve(standard_curve_data, stats_summary, fit_results):
    """
    Create comprehensive plots for the standard curve analysis including fitted line.

    Parameters:
    standard_curve_data (pd.DataFrame): Raw standard curve data
    stats_summary (pd.DataFrame): Statistics summary
    fit_results (dict): Linear fit results
    """
    # Get column names
    # Get measurement column name

  # measurement_col = [col for col in standard_curve_data.columns if col not in ['ID', 'is_numeric', 'Concentration']][0]
    measurement_col = [col for col in standard_curve_data.columns if col not in ['ID', 'is_numeric', 'Concentration', 'Well', 'Row', 'Column']][0]
    mean_col = [col for col in stats_summary.columns if col.startswith('mean_')][0]

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # Plot 1: Raw data points with fitted line
    ax1.scatter(standard_curve_data['Concentration'],
               standard_curve_data[measurement_col],
               alpha=0.6, s=50, label='Data points')

    # Add fitted line
    x_line = np.linspace(standard_curve_data['Concentration'].min(),
                        standard_curve_data['Concentration'].max(), 100)
    y_line = fit_results['predict_measurement'](x_line)
    ax1.plot(x_line, y_line, 'r-', linewidth=2,
             label=f"y = {fit_results['slope']:.4f}x + {fit_results['intercept']:.4f}\nR¬≤ = {fit_results['r_squared']:.4f}")

    ax1.set_xlabel(f'Concentration ({fit_results["concentration_unit"]})')
    ax1.set_ylabel(f'{fit_results["measurement_name"]}')
    ax1.set_title('Standard Curve - All Data Points with Linear Fit')
    ax1.grid(True, alpha=0.3)
    ax1.legend()

    # Plot 2: Mean values with error bars and fitted line
    ax2.errorbar(stats_summary['Concentration'],
                stats_summary[mean_col],
                yerr=stats_summary['std_dev'],
                marker='o', capsize=5, capthick=2, markersize=8,
                label='Mean ¬± SD', fmt='o-')

    # Add fitted line to mean plot
    x_line_mean = np.linspace(stats_summary['Concentration'].min(),
                             stats_summary['Concentration'].max(), 100)
    y_line_mean = fit_results['predict_measurement'](x_line_mean)
    ax2.plot(x_line_mean, y_line_mean, 'r--', linewidth=2,
             label=f"Linear fit (R¬≤ = {fit_results['r_squared']:.4f})")

    ax2.set_xlabel(f'Concentration ({fit_results["concentration_unit"]})')
    ax2.set_ylabel(f'Mean {fit_results["measurement_name"]} ¬± SD')
    ax2.set_title('Standard Curve - Mean Values with Linear Fit')
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # Plot 3: Coefficient of Variation
    bars = ax3.bar(stats_summary['Concentration'].astype(str),
                   stats_summary['cv_percent'],
                   color=['red' if cv > 10 else 'blue' for cv in stats_summary['cv_percent']])
    ax3.set_xlabel(f'Concentration ({fit_results["concentration_unit"]})')
    ax3.set_ylabel('Coefficient of Variation (%)')
    ax3.set_title('Coefficient of Variation by Concentration')
    ax3.tick_params(axis='x', rotation=45)
    ax3.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='10% threshold')
    ax3.legend()

    # Add CV values on bars
    for bar, cv in zip(bars, stats_summary['cv_percent']):
        if not pd.isna(cv):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{cv:.1f}%', ha='center', va='bottom')

    # Plot 4: Residuals plot for fit quality assessment
    residuals = stats_summary[mean_col] - fit_results['predict_measurement'](stats_summary['Concentration'])
    ax4.scatter(stats_summary['Concentration'], residuals, s=100)
    ax4.axhline(y=0, color='red', linestyle='--', alpha=0.7)
    ax4.set_xlabel(f'Concentration ({fit_results["concentration_unit"]})')
    ax4.set_ylabel('Residuals (Observed - Predicted)')
    ax4.set_title('Residuals Plot - Fit Quality Check')
    ax4.grid(True, alpha=0.3)

    # Add residual values as text
    for i, (conc, res) in enumerate(zip(stats_summary['Concentration'], residuals)):
        ax4.text(conc, res + 0.01, f'{res:.3f}', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    plt.show()

def analyze_samples_with_standard_curve(long_format_data, fit_results):
    """
    Use the fitted standard curve to calculate concentrations for sample wells.

    Parameters:
    long_format_data (pd.DataFrame): The complete dataset
    fit_results (dict): Results from linear fit

    Returns:
    pd.DataFrame: Sample data with calculated concentrations
    """

    # Create a copy and remove missing values
    df = long_format_data.copy()

    # Get measurement column name
    measurement_col = 'Absorbance' if 'Absorbance' in df.columns else df.columns[1]

    df = df.dropna(subset=['ID', measurement_col])

    # Function to check if a value is purely numeric
    def is_numeric(value):
        try:
            float(value)
            return True
        except (ValueError, TypeError):
            return False

    # Filter for non-numeric IDs (samples, not standards)
    df['is_numeric'] = df['ID'].apply(is_numeric)
    sample_data = df[df['is_numeric'] == False].copy()

    if len(sample_data) == 0:
        print("No sample data found (all IDs are numeric)")
        return pd.DataFrame(), pd.DataFrame()

    # Convert measurement to numeric
    sample_data[measurement_col] = pd.to_numeric(sample_data[measurement_col])

    # Calculate concentrations using the fitted curve
    sample_data['Calculated_Concentration'] = sample_data[measurement_col].apply(
        fit_results['predict_concentration']
    )

    # Calculate statistics for each sample ID
    sample_stats = sample_data.groupby('ID').agg({
        measurement_col: ['count', 'mean', 'std'],
        'Calculated_Concentration': ['mean', 'std']
    }).round(4)

    # Flatten column names
    sample_stats.columns = ['n_replicates', f'mean_{measurement_col.lower()}', f'std_{measurement_col.lower()}',
                           f'mean_concentration_{fit_results["concentration_unit"]}', f'std_concentration_{fit_results["concentration_unit"]}']
    sample_stats = sample_stats.reset_index()

    # Calculate CV for concentrations
    sample_stats[f'cv_concentration_percent'] = (
        sample_stats[f'std_concentration_{fit_results["concentration_unit"]}'] /
        sample_stats[f'mean_concentration_{fit_results["concentration_unit"]}'] * 100
    ).round(2)

    print(f"\n=== SAMPLE ANALYSIS RESULTS ===")
    print(f"Total sample wells: {len(sample_data)}")
    print(f"Unique samples: {len(sample_stats)}")
    print("\nSample Concentrations:")
    print(sample_stats.to_string(index=False))

    return sample_data, sample_stats

def complete_assay_analysis(long_format_data, concentration_unit='units', measurement_name='Measurement'):
    """
    Complete analysis pipeline for any assay with standard curve.

    Parameters:
    long_format_data (pd.DataFrame): DataFrame with 'ID' and measurement columns
    concentration_unit (str): Unit for concentration (e.g., 'Œºg/ml', 'ng/ml', 'mM')
    measurement_name (str): Name of measurement (e.g., 'Absorbance', 'Fluorescence')

    Returns:
    dict: Complete results including all analysis components
    """

    print(f"üî¨ Starting {measurement_name} Assay Analysis")
    print(f"üìä Concentration unit: {concentration_unit}")
    print("="*50)

    # Step 1: Analyze standard curve
    standard_curve_data, stats_summary = filter_and_analyze_standard_curve(
        long_format_data, concentration_unit, measurement_name
    )

    if len(standard_curve_data) == 0:
        return {"error": "No standard curve data found"}

    # Step 2: Fit linear model
    try:
        fit_results = fit_linear_standard_curve(stats_summary, concentration_unit, measurement_name)
    except Exception as e:
        print(f"‚ùå Error in fitting: {e}")
        return {"error": f"Fitting failed: {e}"}

    # Step 3: Plot results
    plot_standard_curve(standard_curve_data, stats_summary, fit_results)

    # Step 4: Analyze samples
    sample_data, sample_stats = analyze_samples_with_standard_curve(long_format_data, fit_results)

    # Return all results
    results = {
        'standard_curve_data': standard_curve_data,
        'stats_summary': stats_summary,
        'fit_results': fit_results,
        'sample_data': sample_data,
        'sample_stats': sample_stats
    }

    return results

# ============================================================================
# IMPORT DATA
# ============================================================================

if __name__ == "__main__":
    print("üß¨ PLATE READER DATA IMPORT FOR GOOGLE COLAB")
    print("=" * 50)

    # Setup Colab environment
    file_path = setup_colab_environment()

    if file_path:
        # Optional: Preview the file first
        preview_choice = input("\nDo you want to preview the file first? (y/n): ")
        if preview_choice.lower() == 'y':
            preview_excel_file(file_path)

        # Get user input for ranges (or use defaults)
        print("\nüìê Excel Range Configuration:")
        print("Default ranges assume standard 96-well plate layout:")
        print("- Absorbance data: B2:M9 (8 rows √ó 12 columns)")
        print("- Sample ID data: B12:M19 (8 rows √ó 12 columns)")

        use_defaults = input("\nUse default ranges? (y/n): ")

        if use_defaults.lower() != 'y':
            abs_range = input("Enter absorbance range (e.g., B2:M9): ") or 'B2:M9'
            id_range = input("Enter ID range (e.g., B12:M19): ") or 'B12:M19'
        else:
            abs_range = 'B2:M9'
            id_range = 'B12:M19'

        # Import and process data
        print("\nüîÑ Processing data...")
        long_format_data = import_and_reshape_excel_colab(
            file_path,
            abs_range=abs_range,
            id_range=id_range
        )

        if long_format_data is not None:
            # Display results
            print("\nüìä RESULTS SUMMARY:")
            print(f"Total wells: {len(long_format_data)}")
            print(f"Unique samples: {long_format_data['ID'].nunique()}")
            print(f"Absorbance range: {long_format_data['Absorbance'].min():.3f} - {long_format_data['Absorbance'].max():.3f}")

            print("\nüîç First 10 rows of processed data:")
            print(long_format_data.head(10).to_string(index=False))

            # Offer to save data
            save_choice = input("\nSave processed data? (y/n): ")
            if save_choice.lower() == 'y':
                save_to_drive = input("Save to Google Drive? (y/n): ")
                save_data_colab(
                    long_format_data,
                    to_drive=(save_to_drive.lower() == 'y')
                )

            print("\n‚úÖ Data import completed successfully!")
            print("The variable 'long_format_data' contains your processed data.")

        else:
            print("\n‚ùå Data import failed. Please check your file and try again.")

    else:
        print("\n‚ùå No file provided. Please run the script again.")

# FIT AND PLOT RESULTS:
"""
# After you have your long_format_data ready, use one of these:

# For protein assay:
results = complete_assay_analysis(long_format_data,
                                concentration_unit='Œºg/ml',
                                measurement_name='Absorbance')

# For enzyme activity assay:
results = complete_assay_analysis(long_format_data,
                                concentration_unit='mM',
                                measurement_name='Fluorescence')

# For DNA quantification:
results = complete_assay_analysis(long_format_data,
                                concentration_unit='ng/ml',
                                measurement_name='Fluorescence')

# Access results:
fit_equation = results['fit_results']['equation_formatted']
r_squared = results['fit_results']['r_squared']
sample_concentrations = results['sample_stats']
"""

results = complete_assay_analysis(long_format_data,
                                concentration_unit='Œºg/ml',
                                measurement_name='Absorbance')
